{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Project\n",
    "# Data Wrangling with MongoDB\n",
    "### by Dogan Askan                                                \n",
    "### Map Area: Syracuse, NY, United States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Data Tidying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': 279475, 'nd': 328321, 'bounds': 1, 'member': 5549, 'tag': 215590, 'relation': 793, 'way': 35493, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "#We can start investigating the data by counting tags\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "filename=\"syracuse_new-york.osm\"\n",
    "def count_tags(filename):\n",
    "    dic={}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in dic.keys():\n",
    "            dic[elem.tag]=1\n",
    "        else:\n",
    "            dic[elem.tag]+=1\n",
    "    return dic\n",
    "\n",
    "print count_tags(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    }
   ],
   "source": [
    "#It seems that there are 194 unique user id in dataset, we will recheck after importing with a query \n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "filename=\"syracuse_new-york.osm\"\n",
    "\n",
    "def process_map(filename):\n",
    "    \"\"\"It is for checking the unique users\n",
    "    \"\"\"\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == \"node\":\n",
    "            ta = element.attrib['uid']\n",
    "            users.add(ta)\n",
    "        pass\n",
    "    return users\n",
    "\n",
    "users = process_map(filename)\n",
    "pprint.pprint(len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['11',\n",
      "     '298',\n",
      "     '31',\n",
      "     'Center',\n",
      "     'Circle',\n",
      "     'Courts',\n",
      "     'East',\n",
      "     'Path',\n",
      "     'Plaza',\n",
      "     'Rowe',\n",
      "     'Run',\n",
      "     'St',\n",
      "     'Terrace',\n",
      "     'Turnpike',\n",
      "     'West'])\n",
      "set(['13202-1107',\n",
      "     '13204-1243',\n",
      "     '132059211',\n",
      "     '13206-2238',\n",
      "     '13210-1053',\n",
      "     '13210-1203',\n",
      "     '13214-1303',\n",
      "     '132179211',\n",
      "     '13218-1185',\n",
      "     '13219-331',\n",
      "     '13224-1110'])\n"
     ]
    }
   ],
   "source": [
    "#Here, we can check problematic fields such as \"postcode\" and \"streetname\".\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "filename=\"syracuse_new-york.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Highway\", \"Way\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\"St.\": \"Street\",\"Ave\": \"Avenue\",\"Rd.\": \"Road\",\"Rd\": \"Road\"}\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == \"tag\":\n",
    "            if element.attrib['k']==\"addr:street\":\n",
    "                namelist=element.attrib['v'].split(\" \")\n",
    "                if namelist[-1] not in expected:\n",
    "                    users.add(namelist[-1])\n",
    "    return users\n",
    "\n",
    "def process_map_zip(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == \"tag\":\n",
    "            if element.attrib['k']==\"addr:postcode\":\n",
    "                zipcode=element.attrib['v']\n",
    "                if len(zipcode)!=5:\n",
    "                    users.add(zipcode)\n",
    "    return users\n",
    "\n",
    "#audit(filename)\n",
    "pprint.pprint(process_map(filename))\n",
    "pprint.pprint(process_map_zip(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, there are a few unsual street name endings. We can programmatically update some such as \"St\" but there are also things that we cannot update programmatically such as \"11\", \"298\" and \"31\". We can write a code to update it into \"Street\".\n",
    "\n",
    "And, for zip codes, there are some codes longer than 5 digits. Those are technically correct. However we can write a code for consistency to just get the first 5 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of out of range latitudes: 27.14%\n",
      "with min latitude 42.941 and max latitude 43.2189814\n",
      "Percent of out of range longitudes: 36.91%\n",
      "with min longitude -76.375 and max longitude -75.9460041\n"
     ]
    }
   ],
   "source": [
    "#To check the latitude and longitude ranges if those are valid\n",
    "#According to Google Maps limits for Syracuse are (42.98,43.09) in latitude and (-76.21,-76.07) in longitude\n",
    "def process_map_lat(filename):\n",
    "    lats = []\n",
    "    i=0\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == \"node\":\n",
    "            i+=1\n",
    "            if float(element.attrib['lat'])<42.98 or float(element.attrib['lat'])>43.09:\n",
    "                lats.append(element.attrib['lat'])\n",
    "    return lats\n",
    "lats=process_map_lat(filename)\n",
    "print \"Percent of out of range latitudes: \"+str(round((len(lats)/float(i))*100,2))+\"%\"\n",
    "print \"with min latitude \"+str(min(lats))+\" and max latitude \"+str(max(lats))\n",
    "\n",
    "def process_map_lon(filename):\n",
    "    lons = []\n",
    "    i=0\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == \"node\":\n",
    "            i+=1\n",
    "            if float(element.attrib['lon'])<-76.21 or float(element.attrib['lon'])>-76.07:\n",
    "                lons.append(element.attrib['lon'])\n",
    "    return lons\n",
    "lons=process_map_lon(filename)\n",
    "print \"Percent of out of range longitudes: \"+str(round((len(lons)/float(i))*100,2))+\"%\"\n",
    "print \"with min longitude \"+str(max(lons))+\" and max longitude \"+str(min(lons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough out of range ratios look huge, we can still say that max and min values are very close to city center. So, we can ignore these and there is no need to create a function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This code turns .osm into .json by making it more understandable. \n",
    "#And, two functions (update_name and update_zip) update street names and zip codes as we discussed earlier.\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "mapping = { \"St\": \"Street\",\"St.\": \"Street\",\"Ave\": \"Avenue\",\"Rd.\": \"Road\",\"Rd\": \"Road\"}\n",
    "def update_name(name, mapping): #to update street names\n",
    "    namelist=name.split(\" \")\n",
    "    if namelist[-1] in mapping.keys():\n",
    "        namelist[-1]=mapping[namelist[-1]]\n",
    "        name=' '.join(namelist)\n",
    "    return name\n",
    "\n",
    "def update_zip(zipcode): #to update zip codes\n",
    "    if len(zipcode)!=5:\n",
    "        zipcode=zipcode[:5]\n",
    "    return zipcode\n",
    "\n",
    "def shape_element(element):\n",
    "    \"\"\"This is for updating .osm elements to nice .json dictionary\"\"\"\n",
    "    node = {}\n",
    "    address={}\n",
    "    created={}\n",
    "    ad=\"addr:\"\n",
    "    pos=[]\n",
    "    node_refs=[]\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        \"\"\"Part 1 - For type key\"\"\"\n",
    "        if element.tag == \"node\":\n",
    "            node['type']='node'\n",
    "        else:\n",
    "            node['type']='way'    \n",
    "        \"\"\"Part 2 - For address key\"\"\"\n",
    "        for i in element.iter('tag'):\n",
    "            if ad in i.attrib['k']:\n",
    "                if \":\" in i.attrib['k'].replace(ad,\"\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    if i.attrib['k'].replace(ad,\"\")==\"street\":\n",
    "                        address[i.attrib['k'].replace(ad,\"\")]=update_name(i.attrib['v'],mapping)\n",
    "                    elif i.attrib['k'].replace(ad,\"\")==\"postcode\":\n",
    "                        address[i.attrib['k'].replace(ad,\"\")]=update_zip(i.attrib['v'])\n",
    "                    else:\n",
    "                        address[i.attrib['k'].replace(ad,\"\")]=i.attrib['v']\n",
    "            else:    \n",
    "                node[i.attrib['k']]=i.attrib['v']\n",
    "        if address:\n",
    "            node[\"address\"]=address\n",
    "        \"\"\"Part 3 - For pos and created keys\"\"\"\n",
    "        for i in element.iter('node'):\n",
    "            for key, value in i.attrib.items():\n",
    "                if key in CREATED:\n",
    "                    created[key]=value\n",
    "                    k=1\n",
    "                elif key==\"lat\" or key==\"lon\":\n",
    "                    pos.append(float(value))     \n",
    "                else:\n",
    "                    node[key]=value\n",
    "        if created:\n",
    "            node['created']=created\n",
    "        if pos:\n",
    "            node['pos']=pos\n",
    "        \"\"\"Part 4 - For node_refs key\"\"\"        \n",
    "        for i in element.iter('nd'):\n",
    "            node_refs.append(i.attrib['ref'])\n",
    "            l=1\n",
    "        if node_refs:\n",
    "            node['node_refs']=node_refs       \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "output=process_map(\"syracuse_new-york.osm\", pretty = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Data Overview\n",
    "\n",
    "In this part, we can analyze the date by using MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File sizes                                              \n",
    "syracuse_new-york.osm ........... 61.2 MB  \n",
    "syracuse_new-york.osm.json .... 63.7 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314968\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client.examples\n",
    "pprint.pprint(db.syracuse.find().count()) #Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279473\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(db.syracuse.find({\"type\":\"node\"}).count()) #node count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35476\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(db.syracuse.find({\"type\":\"way\"}).count()) #way count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique users\n",
    "> db.syracuse.distinct(\"created.user\").length  \n",
    "194 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'zeromap', u'count': 136837}]\n"
     ]
    }
   ],
   "source": [
    "#Top 1 contributing user\n",
    "def make_pipeline():\n",
    "    pipeline = [{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":1}]\n",
    "    return pipeline\n",
    "\n",
    "def syracuse(db, pipeline):\n",
    "    return [doc for doc in db.syracuse.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = syracuse(db, make_pipeline())\n",
    "    pprint.pprint(result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': 1, u'num_users': 45}]\n"
     ]
    }
   ],
   "source": [
    "#Number of users appearing only once (having 1 post)\n",
    "def make_pipeline():\n",
    "    pipeline = [{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, {\"$group\":{\"_id\":\"$count\", \"num_users\":{\"$sum\":1}}}, \n",
    "                {\"$sort\":{\"_id\":1}}, {\"$limit\":1}]\n",
    "    return pipeline\n",
    "\n",
    "def syracuse(db, pipeline):\n",
    "    return [doc for doc in db.syracuse.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = syracuse(db, make_pipeline())\n",
    "    pprint.pprint(result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'13224', u'count': 821},\n",
      " {u'_id': u'13214', u'count': 514},\n",
      " {u'_id': u'13210', u'count': 475},\n",
      " {u'_id': u'13206', u'count': 290},\n",
      " {u'_id': u'13205', u'count': 284}]\n"
     ]
    }
   ],
   "source": [
    "#Here, we can see top 5 zip codes\n",
    "def make_pipeline():\n",
    "    pipeline = [{\"$match\":{\"address.postcode\":{\"$exists\":1}}}, \n",
    "                   {\"$group\":{\"_id\":\"$address.postcode\", \"count\":{\"$sum\":1}}}, \n",
    "                {\"$sort\":{\"count\":-1}}]\n",
    "    return pipeline\n",
    "\n",
    "def syracuse(db, pipeline):\n",
    "    return [doc for doc in db.syracuse.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = syracuse(db, make_pipeline())\n",
    "    pprint.pprint(result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Erie Boulevard East', u'count': 263},\n",
      " {u'_id': u'Westmoreland Avenue', u'count': 206},\n",
      " {u'_id': u'South Salina Street', u'count': 197},\n",
      " {u'_id': u'East Genesee Street', u'count': 163},\n",
      " {u'_id': u'Westcott Street', u'count': 155}]\n"
     ]
    }
   ],
   "source": [
    "#Here, we can see top 5 street names\n",
    "def make_pipeline():\n",
    "    pipeline = [{\"$match\":{\"address.street\":{\"$exists\":1}}}, \n",
    "                   {\"$group\":{\"_id\":\"$address.street\", \"count\":{\"$sum\":1}}}, \n",
    "                {\"$sort\":{\"count\":-1}}]\n",
    "    return pipeline\n",
    "\n",
    "def syracuse(db, pipeline):\n",
    "    return [doc for doc in db.syracuse.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = syracuse(db, make_pipeline())\n",
    "    pprint.pprint(result[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, this map is incomplete and there many things that can be done such as better street name updates or adding additional a field for last 4 digits of zip codes. Yet again, these improvement will definitely be beneficial to provide more efficient, explainable and faster database quries. Regarding the coordinates, I don't think it is a problem because the closest city to those points is definitely Syracuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'parking', u'count': 939},\n",
      " {u'_id': u'school', u'count': 186},\n",
      " {u'_id': u'fast_food', u'count': 158},\n",
      " {u'_id': u'bench', u'count': 153},\n",
      " {u'_id': u'restaurant', u'count': 152},\n",
      " {u'_id': u'place_of_worship', u'count': 131},\n",
      " {u'_id': u'fuel', u'count': 122},\n",
      " {u'_id': u'bank', u'count': 64},\n",
      " {u'_id': u'post_box', u'count': 57},\n",
      " {u'_id': u'pharmacy', u'count': 51}]\n"
     ]
    }
   ],
   "source": [
    "#Here, we can see top 10 catagories.\n",
    "def make_pipeline():\n",
    "    pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}}}, \n",
    "                   {\"$group\":{\"_id\":\"$amenity\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}]\n",
    "    return pipeline\n",
    "\n",
    "def syracuse(db, pipeline):\n",
    "    return [doc for doc in db.syracuse.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = syracuse(db, make_pipeline())\n",
    "    pprint.pprint(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'parking', u'ratio': 34.77777777777778},\n",
      " {u'_id': u'school', u'ratio': 6.888888888888889},\n",
      " {u'_id': u'fast_food', u'ratio': 5.851851851851852},\n",
      " {u'_id': u'bench', u'ratio': 5.666666666666667},\n",
      " {u'_id': u'restaurant', u'ratio': 5.62962962962963},\n",
      " {u'_id': u'place_of_worship', u'ratio': 4.851851851851852},\n",
      " {u'_id': u'fuel', u'ratio': 4.518518518518518},\n",
      " {u'_id': u'bank', u'ratio': 2.3703703703703702},\n",
      " {u'_id': u'post_box', u'ratio': 2.111111111111111},\n",
      " {u'_id': u'pharmacy', u'ratio': 1.8888888888888888}]\n"
     ]
    }
   ],
   "source": [
    "#Amenity ratios in Syracuse\n",
    "total=db.syracuse.find({\"amenity\":{\"$exists\":1}}).count() #All amenties\n",
    "def make_pipeline():\n",
    "    pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}}},{\"$group\":{\"_id\":\"$amenity\", \"count\":{\"$sum\":1}}}, \n",
    "                {\"$project\":{\"_id\":\"$_id\",\"ratio\":{\"$divide\":[\"$count\",total/100]}}},\n",
    "                {\"$sort\":{\"ratio\":-1}}]\n",
    "    return pipeline\n",
    "\n",
    "def syracuse(db, pipeline):\n",
    "    return [doc for doc in db.syracuse.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = syracuse(db, make_pipeline())\n",
    "    pprint.pprint(result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there that much \"parking\"? It may be some mistake needs to be analyzed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Subway', u'count': 18},\n",
      " {u'_id': u\"McDonald's\", u'count': 15},\n",
      " {u'_id': u\"Dunkin' Donuts\", u'count': 12},\n",
      " {u'_id': u'Burger King', u'count': 11},\n",
      " {u'_id': u\"Wendy's\", u'count': 5},\n",
      " {u'_id': u'Original Italian Pizza', u'count': 4},\n",
      " {u'_id': u'Taco Bell', u'count': 4},\n",
      " {u'_id': u\"Arby's\", u'count': 3},\n",
      " {u'_id': u\"Pavone's Pizza\", u'count': 3},\n",
      " {u'_id': u'KFC', u'count': 3}]\n"
     ]
    }
   ],
   "source": [
    "#Top 10 fast food places, Subway wins!\n",
    "def make_pipeline():\n",
    "    pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"fast_food\"}}, \n",
    "                   {\"$group\":{\"_id\":\"$name\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}]\n",
    "    return pipeline\n",
    "\n",
    "def syracuse(db, pipeline):\n",
    "    return [doc for doc in db.syracuse.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = syracuse(db, make_pipeline())\n",
    "    pprint.pprint(result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing unsual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'zeromap', u'ratio': 48.97530422333572},\n",
      " {u'_id': u'woodpeck_fixbot', u'ratio': 27.032211882605583},\n",
      " {u'_id': u'DTHG', u'ratio': 8.641016463851109},\n",
      " {u'_id': u'RussNelson', u'ratio': 2.6904080171796707},\n",
      " {u'_id': u'yhahn', u'ratio': 2.48067287043665},\n",
      " {u'_id': u'fx99', u'ratio': 1.473514674302076},\n",
      " {u'_id': u'timr', u'ratio': 0.9659985683607731},\n",
      " {u'_id': u'TIGERcnl', u'ratio': 0.7415891195418755},\n",
      " {u'_id': u'Johnc', u'ratio': 0.6717967072297781},\n",
      " {u'_id': u'ECRock', u'ratio': 0.5662133142448104}]\n"
     ]
    }
   ],
   "source": [
    "#Let's check contribution ratios\n",
    "total=db.syracuse.find({\"created.user\":{\"$exists\":1}}).count() #All users\n",
    "def make_pipeline():\n",
    "    pipeline = [{\"$match\":{\"created.user\":{\"$exists\":1}}},{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, \n",
    "                {\"$project\":{\"_id\":\"$_id\",\"ratio\":{\"$divide\":[\"$count\",total/100]}}},\n",
    "                {\"$sort\":{\"ratio\":-1}}]\n",
    "    return pipeline\n",
    "\n",
    "def syracuse(db, pipeline):\n",
    "    return [doc for doc in db.syracuse.aggregate(pipeline)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    result = syracuse(db, make_pipeline())\n",
    "    pprint.pprint(result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first user has almost half of the total contributions. And, top 5 users have the 90%. More active contributors are needed to improve the map. Currently, it looks that it is only actively contributed by a few users. I am sure there are many smart phone and online map users never heard OpenStreetMap yet. I, personally, wasn't aware of it before this course. So, better advertising is essential to provide more active contributors. By doing so, the first problem \"small data\" can be fixed.  \n",
    "\n",
    "In addition to this, data coming from contributors need to be clean and consistent. To do so, for example, OpenStreetMap can limit the zip code attribute as only 5 digits. Or, a recommender system can be utilized for some section to recommend \"Street\" or \"Road\" while \"St\" or \"Rd\" are being written."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
